---
title: "Vignette Title"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The Hathi Trust has made available 15 million volumes of text with word counts at the page level. ^[Boris Capitanu, Ted Underwood, Peter Organisciak, Timothy Cole, Maria Janina Sarol, J. Stephen Downie (2016). The HathiTrust Research Center Extracted Feature Dataset (1.0) [Dataset]. HathiTrust Research Center,http://dx.doi.org/10.13012/J8X63JT3.]

The purpose of this package is to allow you to quickly work with tidy-formatted data for any of these books.
These features are useful input into a wide variety of tasks in natural language processing, visualization, and other areas.


# Gibbon

As an example, let's take Edward Gibbon's *Decline and Fall of the Roman Empire*. 

First, I'll define the corpus as a list of ids.

```{r}
gibbon = c("njp.32101076189404", "njp.32101076189412", "njp.32101076189420", "njp.32101076189438", "njp.32101076189446", "njp.32101076189453","njp.32101076189461", "njp.32101076189479", "njp.32101076189487", "njp.32101076189495", "njp.32101076189503")
```

Next, some imports. `hathidy` is this package: ggplot2 is for visualization, and `dplyr` and `tidytext` together supply a number of useful functions for working with wordcount data. 

```{r, fig.show='hold'}
library(hathidy)
library(ggplot2)
library(dplyr)
library(tidytext)
```

Now we can load the data in. Note the global directory at the front here. You can store features in the current working directory, (probably in a folder called "features"), or use a global one, which is useful if you might work with Hathi more than once. If you don't specify a location, feature counts will be downloaded to a temporary volume, which is probably *not* the best behavior. (Among other things, your code will take much, much longer to run on a second instance.)

```{r}
options(hathidy_dir = "~/ht-ef/")
gibbon_books = hathi_counts(gibbon, cols = c("page", "token", "section"))
gibbon_books
```


```{r}
gibbon_books %>% 
  group_by(htid, token) %>% summarize(count=sum(count)) %>%
  bind_tf_idf(token, htid, count) %>% 
  arrange(-tf_idf) %>% 
  group_by(htid, token) %>%
  summarize(totidf = sum(tf_idf), count=sum(count)) %>%
  arrange(-totidf) %>%
  slice(1:3)
```


This bit doesn't work because POS is no longer requested.
```{r}


library(ggplot2)
library(dplyr)
library(stringr)

data = gibbon_books %>%
  mutate(overall_page = 1:n()) %>%
  add_chunks(count, 10000) %>%
  group_by(token) %>%
  filter(sum(count) > 250) %>% # Only common words
  mutate(wordTotal = sum(count)) %>%
  mutate(lower = tolower((token))) %>%
  group_by(lower) %>%
  mutate(formTotal = sum(count)) %>%
  group_by(token, POS) %>%
  mutate(posTotal = sum(count)) %>%
  filter(str_length(token) > 2, str_detect(token, "^[A-Z][a-z]")) %>%
  filter(wordTotal/formTotal > .95) %>%
  group_by(token) %>%
  filter(sum(posTotal[POS=="NNP"])/wordTotal > .85)

data %>%
  group_by(chunk, token) %>%
  mutate(count=sum(count)) %>%
  ggplot() + # aes(x=overall_page, size = count, y=reorder(token, count, FUN = sum), color=POS) +#geom_point() + theme_bw() + scale_size_continuous(trans="sqrt")
  geom_tile(aes(fill=count, color=NULL, size=NULL, x = chunk, y = reorder(token, count, FUN=sum))) + scale_fill_viridis_c(trans="log")
```

Can we get a better ordering? Yes.

```{r}

umap_ordering = function(frame) {
    mat = frame %>% select(-token) %>% as.matrix
    mat = mat/rowSums(mat)
    d = umap::umap(mat, method="umap-learn", n_neighbors = 8, n_components = 1)
    d$layout[,1]
}

ordering = data %>% group_by(chunk, token) %>% summarize(count=sum(count)) %>%
  tidyr::spread(chunk, count, fill=0) %>%
  mutate(order = umap_ordering(.)) %>%
  select(token, order)



data %>% 
  group_by(chunk, token) %>%
  summarize(count=sum(count)) %>%
  inner_join(ordering) %>%
  ggplot() + aes(x=chunk, size = (count), y=reorder(token, order)) + geom_point() + theme_bw() + scale_size_continuous(trans="sqrt")

  geom_tile(aes(fill=count, color=NULL, size=NULL, x = chunk, y = reorder(token, order, FUN=sum))) + scale_fill_viridis_c(trans="log")


```

```{r}
add_chunks = function(frame, length=2000) {
  # Breaks a book into n-word chunks.
  frame %>%
    group_by(page, add = TRUE) %>% 
    summarize(count=sum(count)) %>% 
    mutate(tally = cumsum(count), chunk = 1 + tally %/% length) %>% 
    select(page, chunk) %>% 
    inner_join(frame, by ="page")
}

library(tidytext)

f %>% 
  add_chunks(10000) %>%
  group_by(token, chunk) %>%
  summarize(count=n()) %>%
  bind_tf_idf(token, chunk, count) %>%
  arrange(-tf_idf)

f %>% arrange(-tfidf)
```
