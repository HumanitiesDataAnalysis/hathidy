---
title: "Vignette Title"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The Hathi Trust has made available 15 million volumes of text with word counts at the page level. ^[Boris Capitanu, Ted Underwood, Peter Organisciak, Timothy Cole, Maria Janina Sarol, J. Stephen Downie (2016). The HathiTrust Research Center Extracted Feature Dataset (1.0) [Dataset]. HathiTrust Research Center,http://dx.doi.org/10.13012/J8X63JT3.]

The purpose of this package is to allow you to quickly work with tidy-formatted data for any of these books.
These features are useful input into a wide variety of tasks in natural language processing, visualization, and other areas.


# Reading historical texts

First, some imports. `hathidy` is this package: ggplot2 is for visualization, and `dplyr` and `tidytext` together supply a number of useful functions for working with wordcount data. 

```{r, fig.show='hold'}
library(hathi)
library(hathidy)
library(ggplot2)
library(dplyr)
library(tidytext)
library(stringr)
```

As an example, let's take Edward Gibbon's *Decline and Fall of the Roman Empire*. 

First, I'll define the corpus as a list of ids.

```{r}
gibbon = c("njp.32101076189404", "njp.32101076189412", "njp.32101076189420", "njp.32101076189438", "njp.32101076189446", "njp.32101076189453","njp.32101076189461", "njp.32101076189479", "njp.32101076189487", "njp.32101076189495", "njp.32101076189503")
```

Now we can load the data in. Note the global directory at the front here. You can store features in the current working directory, (probably in a folder called "features"), or use a global one. If you think you might work with Hathi more than once, having a local location might make sense: but for reproducible research, you should store just the files used in this particular project. If you don't specify *any* location, feature counts will be downloaded to a temporary directory, which is less than ideal. (Among other things, your code will take much, much longer to run on a second instance.)

```{r}
options(hathidy_dir = "~/ht-ef/")
gibbon_books = hathi_counts(gibbon, cols = c("page", "token"))
```

```{r}
gibbon_books %>%
  group_by(page, htid) %>% 
  summarize(count=n()) %>%
  ggplot() + 
  geom_tile(aes(x=page, y = htid, fill = count)) +
  scale_fill_viridis_c()
  
```

A similar plot, but looking through all million words in the decline, can let us see how often Gibbon speaks of "Barbarians." The chart shows a major change in verbiage: Gibbon stops heavily using the phrase 'barbarian' after the first book (first two of 12 volumes.)

```{r}
gibbon_books %>% 
  ungroup %>%
  mutate(overall_position = 1:n()) %>%
  filter(token %>% str_detect("barbar")) %>%
  ggplot() + geom_point(aes(x=overall_position, y = token, color=htid), alpha = 0.33, position="jitter") + 
  labs(title="Usage of 'barbarian' in Gibbon's 'Decline'", subtitle="Book 1 uses 'Barbarian' much more")
```


```{r}
gibbon_books %>% 
  group_by(htid, token) %>% summarize(count=sum(count)) %>%
  bind_tf_idf(token, htid, count) %>% 
  filter(str_detect(token, "^[a-z]{4,}$")) %>% # Only lowercase words; remove to see proper names.
  arrange(-tf_idf) %>%
  slice(1:3)
```

```{r}
f = gibbon_books %>%
  group_by(htid)

z = f %>%
  group_by(htid) %>%
  add_chunks(count, 10000) %>%
  group_by(htid, chunk) %>%
  grouped_tfidf(token, count)

z %>% 
  filter(token %>% str_detect("^[A-Za-z]+$")) %>%
  arrange(-tfidf)

```


Should you want to explore plotting on a one-dimensional axis using UMAP, try the below code.

```{r}
if (FALSE) {}
umap_ordering = function(frame) {
    mat = frame %>% select(-token) %>% as.matrix
    mat = mat/rowSums(mat)
    d = umap::umap(mat, n_neighbors = 8, n_components = 1)
    d$layout[,1]
}

ordering = data %>% 
  mutate(doc = paste(htid, chunk)) %>%
  group_by(doc, token) %>% 
  summarize(count=sum(count)) %>%
  tidyr::spread(doc, count, fill=0) %>%
  mutate(order = umap_ordering(.)) %>%
  select(token, order)

data %>% 
  group_by(chunk, token) %>%
  summarize(count=sum(count)) %>%
  inner_join(ordering) %>%
  ggplot() + aes(x=chunk, size = (count), y=reorder(token, order)) + geom_point() + theme_bw() + scale_size_continuous(trans="sqrt")

  geom_tile(aes(fill=count, color=NULL, size=NULL, x = chunk, y = reorder(token, order, FUN=sum))) + scale_fill_viridis_c(trans="log")

}
```

